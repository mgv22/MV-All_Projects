---
title: "Assignment 2"
---

```{r knitr_options, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


```{r setup, include=FALSE}
US_States <- read_csv("~/Desktop/Graduate School/Supervised Learning/Module 2/USStates.csv")

# Create subset without demographic variables
Subset_df <- US_States %>%
  select(-c("State", "Region", "Population"))

```


# EDA  
## Summary Statistics
Summary statistics were obtained for the 10 continuous variables included in the dataset. The results can been in the table below.

```{r}
# Function to compute summary statistics for all continuous variables
function1 <- function(x) {
  c(
    "Stand dev" = round(sd(x, na.rm = TRUE), digits = 2),
    "Mean" = round(mean(x, na.rm = TRUE), digits = 2),
    "Median" = round(median(x), digits = 2),
    "Minimum" = round(min(x, na.rm = TRUE), digits = 2),
    "Maximum" = round(max(x, na.rm = TRUE), digits = 2)
  )
}

# Apply  the function to subset of data (remove demographic fields)
Numeric <- US_States %>%
  select(-c("State", "Region", "Population")) %>%
  map( ~ function1(.)) %>%
  as.data.frame()


# Retrieve Row Names
row_names <- row.names(Numeric)

# Re-combine the dataframe in more readable format
Summary_Numeric <- cbind(Numeric, row_names) %>%
  gather(key = Variable, value = value, -row_names) %>%
  spread(key = row_names, value = value)


# Table output
knitr::kable(Summary_Numeric)
```
## Scatterplot Matrix  
Each of the 9 continuous explanatory variables were plotted against the response variable (HouseholdIncome) as a first step in determining if there was any noticeable relationship. Several of the explanatory variables have a linear relationship with HouseholdIncome. Smokers seems to be negatively correlated with HouseholdIncome while College has a strong positive linear correlation.


```{r}
par(mfrow = c(3, 3))

for (i in 2:length(names(Subset_df))) {
  plot(
    x = Subset_df[[i]],
    y = Subset_df[[1]],
    main = paste(names(Subset_df[, 1]), "vs", names(Subset_df[, i])),
    xlab = names(Subset_df[, i]),
    ylab = names(Subset_df[, 1]),
    col = "dark green",
    pch = 16,
    cex = 2
  )
}
```

```{r}
# Create correlation dataframe
Correlation_df <- Subset_df %>%
  mutate(
    HI = HouseholdIncome,
    HS = HighSchool,
    CL = College,
    SM = Smokers,
    PA = PhysicalActivity,
    OB = Obese,
    NW = NonWhite,
    HD = HeavyDrinkers,
    TP = TwoParents,
    IN = Insured
  ) %>%
  select(HI, HS, CL, SM, PA, OB, NW, HD, TP, IN)
Sub_Cor <- round(cor(Correlation_df, method = "pearson"), digits = 2)

# Print the df using kable
knitr::kable(Sub_Cor)
```

With that in mind, we can see from the correlation table above that we have several strong linear relationships with our response variable (HouseholdIncome). College has the strongest positive correlation (0.69) while Obese has the strongest negative (-0.65). Accompanied with our visual representations of the relationships above, we can say that this data is appropriate for linear modeling. None of the variables have obvious non-linear relationships with the response variable as shown in the scatter plots. The correlation table confirms that there are strong linear relationships between HouseholdIncome and several variables.


# Modeling  
## Model 1  
The first linear model consists of the explanatory variable College and the response variable HouseholdIncome. College is an appropriate first variable to include in a linear model due to it having the highest linear correlation with the response variable.
### Model Summary  
Below we can see a comprehensive summary of Model 1. Several of the components from the summary will be used to understand and interpret the model in the following sections.


```{r}
# Fit the model
model1 <- lm(HouseholdIncome ~ College, data = US_States)
# Model coefficients
summary(model1)


```

### Model Coefficients & Equation
Using these coefficients, we can represent the ordinary least squares linear model with the following formula:  
$$
\hat{Y}=23.0664+0.9801X
$$
The value 23.0664 in the above equation represents the the Y-intercept or the value of $Y$ when $X=0$. In the scope of our analysis, this means that the value of HouseholdIncome is 23.0664 when our College variable is equal to 0. If our population has no one with College education we can say the average HouseholdIncome is equal to 23.0664. This minimum household income in the dataset is 39, so this intercept value doesn't seem feasible. The intercept coefficient also has a standard error of 4.7187 with a corresponding t-value of 4.888. The t-value allows us to reject the null hypothesis that our intercept is equal to 0. Logically, an intercept of 0 wouldn't make sense as it would imply that our population had no household income unless they had a college education. .

The regression coefficient for the College variable is 0.9801. This means that for every 1 unit increase in College value, we can expect a 0.9801 unit increase in HouseholdIncome. The values in the dataset are considered averages or proportions that are representative of the population. With that in mind, the model is saying that if the proportion of the population with College education increases by 1, the HoueholdIncome should increase by 0.9801. As the population of a state becomes more educated, their income potential increases. This is a reasonable variable to include in future models.


```{r}
#ANOVA Table
anova(model1)
```
Our ANOVA table shows an F-value of 42.572 that allows us to reject the null hypothesis that our model coefficients are equal to 0.

```{r}
### R Squared
model1_rsquared <- summary(model1)$r.squared
model1_rsquared
```
The r-squared value for Model1 is `r model1_rsquared`. This means that approximately $47\%$ of the variance in $Y$ is explained with $X$.

```{r}
### Prediction & Residual Computations

# Predicted values
US_States$m1_pred <- predict.lm(model1, US_States)
# Residuals
US_States$m1_residual <- US_States$HouseholdIncome - US_States$m1_pred
# Squared Residuals
US_States$m1_resid_squared <- US_States$m1_residual ^ 2
# Sum of Squared Residuals
SSE <- sum(US_States$m1_resid_squared)
# Sum of Squares Total
US_States$m1_mean_deviate <-
  US_States$HouseholdIncome - mean(US_States$HouseholdIncome)
US_States$m1_mean_deviate_sq <- US_States$m1_mean_deviate ^ 2
SST <- sum(US_States$m1_mean_deviate_sq)
# Sum of Squares due to Regression
US_States$m1_hat_deviate_bar <-
  US_States$m1_pred - mean(US_States$HouseholdIncome)
US_States$m1_hat_deviate_sq <- US_States$m1_hat_deviate_bar ^ 2
SSR <- sum(US_States$m1_hat_deviate_sq)
```
The following values were calculated manually in order to validate the output results from Model1. 

*Sum of Squared Residuals = `r round(SSE, digits = 2)`*  
*Sum of Squares Total = `r round(SST, digits = 2)`*  
*Sum of Squares due to Regression = `r round(SSR, digits = 2)`*  
*R Squared = `r round(SSR / SST, digits = 2)`*  

If we look at the output from our ANOVA table in the section above, we can see that we've correctly calculated the values.

## Model 2  
Model 2 was built using HouseholdIncome as the response variable and **College** and **Insured** as the explanatory variables.  
### Model Summary  
We can see the comprehensive summary from our model output below.  

```{r}
model2 <- lm(HouseholdIncome ~ College + Insured, data = US_States)
summary(model2)
anova(model2)
```

### Model Coefficients & Equation  
Using the coefficients from the summary above, the OLS regression equation is as follows:  
$$
\hat{Y}=9.6728+0.8411X_{1}+0.2206X_{2}
$$
The intercept coefficient is now 9.6728. This means that if all explanatory values are equal to zero, the average HouseholdIncome would be equal to 9.6728. This value can not be used in the model. The standard error is 14.8628 with an associated t-value of 0.651. There is not enough evidence to reject the null hypothesis that the intercept equals 0. The College regression coefficient is equal to 0.8411 which indicates that a 1 unit increase in the College proportion will result in a 0.8411 increase in average Household Income when all other variables are held constant. The coefficient for the College variable has decreased, but it is still considered statistically significant to the model with a p-value of 0.000216. The Insured coefficient is 0.2206. This means that for a 1 unit increase in the proportion of people insured, the average household income will increase 0.2206. 

This regression coefficient is not a meaningful addition to the model. The standard error is greater than the coefficient value indicating that the coefficient could in fact be of the opposite sign or zero. Because of the t-value of the coefficient, we can not reject the null hypothesis that it is zero. This variable should not be retained for future models.  

### R Squared  
The r-squared value for Model2 is 0.48. This is an increase of **0.01** from Model1 (0.47). This means that by adding the Insured variable, our model was only able to account for an additional $1\%$ of the variance in household income. Seeing as how adding variables inherently increases the r-squared value, the addition of Insured as an explanatory variable hasn't yielded anything meaningful.

```{r}

## Additional Models  
model3 <-
  lm(HouseholdIncome ~ College + Insured + HighSchool, data = US_States)
m3_r <- summary(model3)$r.squared
model4 <-
  lm(HouseholdIncome ~ College + Insured + HighSchool + Smokers, data = US_States)
m4_r <- summary(model4)$r.squared
model5 <-
  lm(HouseholdIncome ~ College + Insured + HighSchool + Smokers + PhysicalActivity,
     data = US_States)
m5_r <- summary(model5)$r.squared
model6 <-
  lm(
    HouseholdIncome ~ College + Insured + HighSchool + Smokers + PhysicalActivity + Obese,
    data = US_States
  )
m6_r <- summary(model6)$r.squared
model7 <-
  lm(
    HouseholdIncome ~ College + Insured + HighSchool + Smokers + PhysicalActivity + Obese + NonWhite,
    data = US_States
  )
m7_r <- summary(model7)$r.squared
model8 <-
  lm(
    HouseholdIncome ~ College + Insured + HighSchool + Smokers + PhysicalActivity + Obese + NonWhite + HeavyDrinkers,
    data = US_States
  )
m8_r <- summary(model8)$r.squared
model9 <-
  lm(
    HouseholdIncome ~ College + Insured + HighSchool + Smokers + PhysicalActivity + Obese + NonWhite + HeavyDrinkers + TwoParents,
    data = US_States
  )
m9_r <- summary(model9)$r.squared

# Summary table of r-squared
model_compare_df <- tibble(
  "Variables" = c(
    "CL + IN + HS",
    "CL + IN + HS + SM",
    "CL + IN + HS + SM + PA",
    "CL + IN + HS + SM + PA + OB",
    "CL + IN + HS + SM + PA + OB + NW",
    "CL + IN + HS + SM + PA + OB + NW + HD",
    "CL + IN + HS + SM + PA + OB + NW + HD + TP"
  ),
  "R-Squared" = c(m3_r, m4_r, m5_r, m6_r, m7_r, m8_r, m9_r)
)
knitr::kable(model_compare_df)
```

Utilizing the abbreviated column names from previous sections, we can see the effect additional explanatory variables have on the r-squared value. There is a drastic increase in the explained variability of household income when incorporating *Smokers* and *NonWhite* as explanatory variables. I believe that the variables College, Smokers and NonWhite should be retained for the final model. Their additions to the model yielded the biggest increase in the r-squared value. Intuitively, it makes sense that people who have gone to college on average make more money. It's also believable that on average, smokers and minority individuals make less.
```{r}
print(model3$coefficients)
print(model4$coefficients)
print(model5$coefficients)
print(model6$coefficients)
print(model7$coefficients)
print(model8$coefficients)
print(model9$coefficients)
```


I observed some very odd things happening to the coefficients as I added additional explanatory variables. Some models ended up having a negative intercept coefficient which is impossible in this particular scenario. It's impossible for a state to have a negative average household income. Additionally, as more variables were added, some coefficients became negative while others were no longer statistically significant (i.e. can not reject the null hypothesis). All of this means that there would be a large amount of error in the model due to high error in the coefficients.

## Model 3  
The final model was fitted using College, Smokers, and NonWhite as explanatory variables.  
### Model Summary  
The summary of the final model can be seen below.
```{r}
last_model <- lm(HouseholdIncome ~ College + Smokers + NonWhite, data = US_States)
summary(last_model)
anova(last_model)
```


### Model Coefficients & Equation  
With the coefficients from the model summary, the regression equation is as follows:  
$$
\hat{Y}=42.71490+0.76050X_{1}-0.84743X_{2}+0.15762X_{3}
$$
The intercept coefficient is 42.71490. This means that if all of the explanatory variables are equal to 0 then the average household income is 42.71490. This coefficient has a corresponding t-value of 4.972 which allows us to reject the null hypothesis that it's zero. This intercept value is believable as the minimum household income value is 39. The College coefficient equals 0.76050. As the proportion of the population who've attended college increases 1 percent, the average household income will increase 0.76050. The Smokers regression coefficient equals -0.84743. As the proportion of the population who smoke increases 1 percent, the average household income will decrease by 0.84743 units. The NonWhite regression coefficient equals 0.15762. I will admit that I found this particular coefficient to be counter intuitive. This means that as the proportion of minorities increases, the average household income increases. 

### R Squared  
The r-squared value for the final model is `r summary(final_model)$r.squared`. The model has accounted for $64\%$ of the variance in household income. The peak r-squared that our model achieved when all variables were used was 0.73.  
I think that it's necessary to re-fit the model with these variables because they appear to have the biggest impact on explaining the variance in the response variable. The additional variables don't add much value to the model and increase the chance for unaccounted interaction effects.  


# Conclusion & Reflection  
Overall, there are several promising variables in the dataset that can help us understand the average household income. From the analysis performed, I can conclude that a well educated and relatively healthy population has a higher average household income. Populations that have higher rates of unhealthy behavior like smoking tend to have lower average household income. I would recommend that preventative health programs be put in place to prevent individuals from taking up smoking as well as educating those who already smoke on it's dangers to their health and long-term financial potential. If a population is generally unhealthy and has lower average household income, then there are typically higher associated healthcare costs. I would also recommend encouraging individuals to attend college in some capacity as it helps their future earning potential.  

Overall, I feel like I learned a great deal from this assignment. I've never really approached modeling from the perspective of conducting a hypothesis test on the actual regression coefficients. One thing that I found to be somewhat incorrect about this analysis was the fact that we performed the model fits on the entire dataset. I suspect that several of the models would not generalize well to unseen data due to the high standard error in some of the explanatory variable's coefficients. I understand that the goal of this assignment was to learn and apply the regression equation to a dataset, so I took that into mind.













